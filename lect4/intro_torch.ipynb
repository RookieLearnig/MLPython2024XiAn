{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Torch Tensors\n",
    "\n",
    "- In order to do machine learning, objects in real life must be transformed to numbers.\n",
    "- This numbers are usually grouped in structures, so we need to store them together in some particular order\n",
    "\n",
    "## Tensors\n",
    "A tensor is a multidimensional array.\n",
    "- Is the basic building block of most deep learning frameworks\n",
    "\n",
    "Tensors can have from 0 to n dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# scalars\n",
    "t = torch.tensor(3)\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D tensor\n",
    "\n",
    "t = torch.tensor([2, 3, 4])\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([2, 3, 4, 5, 6, 7, 8])\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D tensor\n",
    "\n",
    "t = torch.tensor(\n",
    "    [\n",
    "        [2, 3, 4], \n",
    "        [6, 7, 8]\n",
    "    ])\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher dimensional tensors\n",
    "t = torch.randn((3, 4, 5))\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher dimensional tensors\n",
    "t = torch.randn((2, 3, 4, 5))\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are homogeneous, so they allow to have a very compact representation in memory, and allow to perform really fast operations.\n",
    "\n",
    "## Indexing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10]\n",
    "    ])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:,1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1,::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can declare the type of a tensor. If not, Torch will infer it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([2, 3, 4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([2.5, 3, 4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([2, 3, 4], dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can convert the tensor type using to() method\n",
    "torch.tensor([2, 3, 4]).to(torch.double)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods you can use to operate tensors. Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(0, 12)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view((6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view((2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = a.view((-1, 4))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(t)\n",
    "display(t.unsqueeze(0))\n",
    "t.shape, t.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(t)\n",
    "display(t.unsqueeze(1))\n",
    "t.shape, t.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(t)\n",
    "display(t.unsqueeze(2))\n",
    "t.shape, t.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(t)\n",
    "display(t.unsqueeze(0))\n",
    "display(t.unsqueeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these operations in tensors do not change the memory representation of the data, so they are very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world data representation using tensors\n",
    "\n",
    "## Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/heart.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('data/heart.csv')\n",
    "\n",
    "df = pd.get_dummies(df).astype(float)\n",
    "\n",
    "# Convert the DataFrame to a PyTorch tensor\n",
    "data_tensor = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "data_tensor.shape, data_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread('images/noidea.jpg')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_channel = image[:,:,0]\n",
    "g_channel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(g_channel, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[:,:,1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[:,:,2], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning, we have the convention of storing the channels first, and then the rows, and then the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(image)\n",
    "out = img.permute(2, 0, 1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this now representation cannot be directly visualized, because it uses a different convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I want to store a batch of images?\n",
    "- Use another dimension for storing the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = out.unsqueeze(0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing videos in tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_frames(video_path, k):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Could not open video file\")\n",
    "    \n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    \n",
    "    while frame_count < k:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Break if there are no more frames to read\n",
    "        \n",
    "        # Convert frame from BGR (OpenCV format) to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Append the frame to the list\n",
    "        frames.append(frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    # Convert the list of frames to a 4D tensor (N, H, W, C) and then to (N, C, H, W)\n",
    "    frames_tensor = torch.tensor(frames, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    \n",
    "    return frames_tensor\n",
    "\n",
    "# Example usage\n",
    "video_path = 'images/funny.mp4'\n",
    "k = 10\n",
    "frames_tensor = read_video_frames(video_path, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to master torch tensor transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize all the values in the images\n",
    "frames_tensor[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = frames_tensor / 255.0\n",
    "normalized[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all the frames to grayscale\n",
    "frames_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_tensor.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_tensor.mean(dim=1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the normalized tensor, convert all the values below a threshold to zero\n",
    "normalized[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "normalized[normalized < threshold] = 0\n",
    "normalized[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** Conventions are really important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
