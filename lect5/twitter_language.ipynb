{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head data/twitter_training.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "twits_raw = pd.read_csv(\"data/twitter_training.csv\", header=None, \n",
    "            names=['id', 'category','polarity', 'text'])\n",
    "twits_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twits_raw.shape)\n",
    "twits_raw =  twits_raw.dropna()\n",
    "print(twits_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = twits_raw.loc[0, 'text']\n",
    "some_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_and_clean(text):\n",
    "    words = re.split(r'\\s+', text)\n",
    "    cleaned_words = [re.sub(r'[^a-zA-Z]', '', word) for word in words]\n",
    "    cleaned_words = [word.lower() for word in cleaned_words if word]\n",
    "    return cleaned_words\n",
    "\n",
    "split_and_clean(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [w for t in twits_raw.text for w in split_and_clean(t)]\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "voc_counter = Counter(vocabulary)\n",
    "voc_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [word for word, counter in voc_counter.items() if counter > 10]\n",
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=20, token_pattern=r'\\b[a-zA-Z]+\\b')\n",
    "X = vectorizer.fit_transform(twits_raw['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "bag_of_words_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)  # Select the number of components you want to keep\n",
    "principal_components = pca.fit_transform(bag_of_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a new DataFrame with the principal components\n",
    "principal_df = pd.DataFrame(principal_components, columns=['PC1', 'PC2'])\n",
    "principal_df['polarity'] = twits_raw.polarity\n",
    "\n",
    "sns.scatterplot(data=principal_df, x='PC1', y='PC2', hue='polarity', palette='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_COMPONENTS = 100\n",
    "\n",
    "pca = PCA(n_components=PCA_COMPONENTS)  # Select the number of components you want to keep\n",
    "principal_components = pca.fit_transform(bag_of_words_df)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a dataset for training a neural network on those data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = principal_components\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y = label_encoder.fit_transform(twits_raw.polarity)\n",
    "y[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float), \n",
    "    torch.tensor(y_train, dtype=torch.long))\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float), \n",
    "    torch.tensor(y_test, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = next(iter(train_loader))\n",
    "data.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(PCA_COMPONENTS, 20)\n",
    "        self.layer2 = nn.Linear(20, 20)\n",
    "        self.layer3 = nn.Linear(20, 4)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = self.layer1(xs).tanh()\n",
    "        x = self.layer2(x).tanh()\n",
    "        x = self.layer3(x)\n",
    "        # x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "mymodel = MyModel().to(device)\n",
    "sum([n.nelement() for n in mymodel.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = mymodel(data.to(device))\n",
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.CrossEntropyLoss()(predicted, label.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mymodel.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    mymodel.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = mymodel(X_batch)\n",
    "        loss = loss_fn(outputs, y_batch)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "print(\"Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "mymodel.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        outputs = mymodel(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with all the parameters, but the accuracy might not significantly improves. So processing text need other ideas in order to perform correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
