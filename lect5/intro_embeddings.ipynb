{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Embeddings\n",
    "\n",
    "Lets generate some names using the previous characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "all_chars = ['.'] + sorted(list(set(\"\".join(words))))\n",
    "itos = {idx: v for idx, v in enumerate(all_chars)}\n",
    "stoi = {v: k for k, v in itos.items()}\n",
    "\n",
    "NUM_CHARS = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use three characters in order to generate the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(314)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.7 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xte, Yte = build_dataset(words[n1:])\n",
    "\n",
    "print(Xtr.shape, Ytr.shape)\n",
    "print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 training instances\n",
    "Xtr[:5], Ytr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets perform a one-hot encoding of each character\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "encoded = F.one_hot(Xtr[:5], num_classes=NUM_CHARS).to(torch.float)\n",
    "Xtr[:5].shape, encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded[0,0], encoded[3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, in order to feed a linear layer, I need to put all one-hot encoded vectors together\n",
    "encoded.view(5, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model for training\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.layer1 = nn.Linear(81, 20)\n",
    "        self.layer2 = nn.Linear(20, NUM_CHARS)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = F.one_hot(xs, num_classes=NUM_CHARS).to(torch.float)\n",
    "        x = x.view(xs.shape[0], -1)\n",
    "        x = self.layer1(x).tanh()\n",
    "        x = self.layer2(x)\n",
    "        # x = self.softmax(x) ... CrossEntropyLoss already contains softmax\n",
    "        return x\n",
    "    \n",
    "model1 = Model1()\n",
    "sum([n.nelement() for n in model1.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.shape, Xtr.view(Xtr.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propabilities for next character for first 5 examples\n",
    "predicted = model1(Xtr[:5])\n",
    "predicted.shape, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "Ytr[:5], loss_fn(predicted, Ytr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random cross entropy loss\n",
    "import numpy as np\n",
    "-np.log(1/NUM_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.01)\n",
    "\n",
    "Xtr_dev = Xtr.to(device)\n",
    "Ytr_dev = Ytr.to(device)\n",
    "model1_dev = model1.to(device)\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model1_dev.train()\n",
    "\n",
    "    outputs = model1_dev(Xtr_dev)\n",
    "    loss = loss_fn(outputs, Ytr_dev)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "model1_dev.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    output_test = model1(Xte.to(device))\n",
    "    loss = loss_fn(output_test, Yte.to(device))\n",
    "    \n",
    "print(f\"Testing loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using embeddings\n",
    "\n",
    "Embedding is a way to code a one-hot enconding into an smaller space, by learning the transformation together with the training procedure.\n",
    "\n",
    "Is like PCA, but:\n",
    "- Takes into account the expected output\n",
    "- Learns while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr[:5], encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 10\n",
    "embedding = nn.Linear(NUM_CHARS, EMBED_SIZE, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply the embedding individually to each encoded character, using the same embedding for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The linear transforms the 27 one-hot encoding into an embedding of 10 dimensions\n",
    "embedding(encoded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other components of the model remain similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.embedding = nn.Linear(NUM_CHARS, EMBED_SIZE, bias=False)\n",
    "        self.layer1 = nn.Linear(3 * EMBED_SIZE, 32)\n",
    "        self.layer2 = nn.Linear(32, NUM_CHARS)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = F.one_hot(xs, num_classes=NUM_CHARS).to(torch.float)\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(xs.shape[0], -1)\n",
    "        x = self.layer1(x).tanh()\n",
    "        x = self.layer2(x)\n",
    "        # x = self.softmax(x) ... CrossEntropyLoss already contains softmax\n",
    "        return x\n",
    "    \n",
    "model2 = Model2()\n",
    "sum([n.nelement() for n in model2.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model2(Xtr[:5])\n",
    "predicted.shape, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model2.parameters(), lr=0.01)\n",
    "\n",
    "Xtr_dev = Xtr.to(device)\n",
    "Ytr_dev = Ytr.to(device)\n",
    "model2_dev = model2.to(device)\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model1_dev.train()\n",
    "\n",
    "    outputs = model2_dev(Xtr_dev)\n",
    "    loss = loss_fn(outputs, Ytr_dev)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system is now more accurate, and you can keep playing with parameters.\n",
    "\n",
    "If we create the embedding in 2D, we can show it in a figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 2\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model3, self).__init__()\n",
    "        self.embedding = nn.Linear(NUM_CHARS, EMBED_SIZE, bias=False)\n",
    "        self.layer1 = nn.Linear(3 * EMBED_SIZE, 50)\n",
    "        self.layer2 = nn.Linear(50, NUM_CHARS)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = F.one_hot(xs, num_classes=NUM_CHARS).to(torch.float)\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(xs.shape[0], -1)\n",
    "        x = self.layer1(x).tanh()\n",
    "        x = self.layer2(x)\n",
    "        # x = self.softmax(x) ... CrossEntropyLoss already contains softmax\n",
    "        return x\n",
    "    \n",
    "model3 = Model3()\n",
    "sum([n.nelement() for n in model3.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model3.parameters(), lr=0.01)\n",
    "\n",
    "Xtr_dev = Xtr.to(device)\n",
    "Ytr_dev = Ytr.to(device)\n",
    "model3_dev = model3.to(device)\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model1_dev.train()\n",
    "\n",
    "    outputs = model3_dev(Xtr_dev)\n",
    "    loss = loss_fn(outputs, Ytr_dev)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.embedding.weight.shape, model3.embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    coords = model3.embedding.weight.cpu() @ torch.eye(NUM_CHARS)\n",
    "coords.shape, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = coords[0].numpy()\n",
    "y = coords[1].numpy()\n",
    "plt.scatter(x, y)\n",
    "for i in range(NUM_CHARS):\n",
    "    plt.annotate(itos[i], (x[i], y[i]), textcoords=\"offset points\", xytext=(5,5), ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
