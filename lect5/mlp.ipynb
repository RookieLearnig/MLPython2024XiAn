{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to add more context we could add more letters to improve the predictions. Nevertheless, the numbers of rows in the contingency table will grow exponentially. \n",
    "\n",
    "Solution: Use the NN approach\n",
    "\n",
    "We will implement a MLP model, based on the first paper that proposed a similar algorithm:\n",
    "[Open PDF](papers/BengioDucharmeVincentJanvin2003.pdf)\n",
    "\n",
    "The idea of the paper is:\n",
    "- Use multiple inputs simultaneously\n",
    "- Use an embedding to encode the inputs (in the paper, words). \n",
    "    - Significant lower dimension than one-hot encoding\n",
    "    - The same encoder is used for all characthers\n",
    "    - Saves a lot of parameters to learn\n",
    "- Use a single hidden fully connected layer with tanh as activation function\n",
    "- Use a softmax output layer for the next character probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "all_chars = ['.'] + sorted(list(set(\"\".join(words))))\n",
    "itos = {idx: v for idx, v in enumerate(all_chars)}\n",
    "stoi = {v: k for k, v in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, they translate from the original 17'000 words space into 30, 60 or 100 space. \n",
    "\n",
    "Here, we will move from the 27 characters space to a 2 dimmensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial code for character at position 5\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alternative one-hot encoding of 5 \n",
    "F.one_hot(torch.tensor(5), num_classes=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the way multiplication is performed, this is equivalent to the direct indexing.\n",
    "\n",
    "Then, we can understand the embeding operation like the first neural network layer\n",
    "- Linear layer where the proper encoding is learned by backpropagation.\n",
    "- No activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to implement the layer, we can use torch indexing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[[5, 6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even repeat indexes, and it dupplicate the rows\n",
    "C[[3, 4, 5, 5, 5, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also index with multidimensional arrays\n",
    "C[torch.tensor([[2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[torch.tensor([[2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, we can directly use the vector X to index in the embedding matrix\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create the embedding layer\n",
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets create the fully connected layer (the one with tanh activation)\n",
    "# the number of inputs is 2 x 3 = 6, and we will use 100 neurons\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have a problem, because the embedding matrix cannot be multiplied by the first layer\n",
    "emb @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem, the embedding results are split in three different component \n",
    "# We need to concatenate the output off all the embeddings [32, 3, 2] -> [32, 6]\n",
    "# Torch has many different functions that can perform that operation\n",
    "emb[:, 0, :].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another posibility is to use torch.unbind, which removes a dimension from a tensor, returning a list\n",
    "torch.cat(torch.unbind(emb, 1), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.all(torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], dim=1) == torch.cat(torch.unbind(emb, 1), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simpler operator is view, which reshape dynamically the informatio of the tensor\n",
    "emb.view(-1, 6).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of both operations are equivalent\n",
    "torch.all(torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], dim=1) == emb.view(-1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the original code\n",
    "h = (emb.view(-1, 6) @ W1 + b1)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got the output of each of the 100 neurons for each of the 32 inputs\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets add the tanh\n",
    "h = (emb.view(-1, 6) @ W1 + b1).tanh()\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the final layer\n",
    "W2 = torch.randn([100, len(all_chars)])\n",
    "b2 = torch.rand(len(all_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And calculate the softmax\n",
    "logits = h @ W2 + b2\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the loss\n",
    "loss = -probs[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a Torch function that performs softmax with the cross entropy loss\n",
    "F.cross_entropy(logits, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not only clearer, but faster, because derivatives are calculate directly (like in minigrad example of tanh)\n",
    "\n",
    "Lets put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put all together\n",
    "g = torch.Generator().manual_seed(31416)\n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn([100, len(all_chars)], generator=g)\n",
    "b2 = torch.rand(len(all_chars), generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10000):\n",
    "\n",
    "    # forward pass\n",
    "\n",
    "    # - Embedding\n",
    "    emb = C[X]\n",
    "    # - Layer 1\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    # - Output layer\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    if epoch < 10 or epoch % 1000 == 0:\n",
    "        print(f\"Epoch:{epoch}, loss={loss.item()}\")\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= 0.01 * p.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The loss go down so fast because we are only overfitting the model of thousands parameters for only 32 objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few errors, mostly because there are identical inputs with different outputs.\n",
    "\n",
    "Now, lets try with the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X, device=device)\n",
    "Y = torch.tensor(Y, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put all together\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((27, 2), generator=g, device=device)\n",
    "W1 = torch.randn((6, 100), generator=g, device=device)\n",
    "b1 = torch.randn(100, generator=g, device=device)\n",
    "W2 = torch.randn([100, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    # forward pass\n",
    "\n",
    "    # - Embedding\n",
    "    emb = C[X]\n",
    "    # - Layer 1\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    # - Output layer\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    if epoch < 10 or epoch % 10 == 0:\n",
    "        print(f\"Epoch:{epoch}, loss={loss.item()}\")\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= 0.1 * p.grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is going down, but we can see each execution is quite slower than before.\n",
    "- Now the loss expansion is a quite large and complex graph, with more than 200K elements!\n",
    "\n",
    "Solution: Run on mini-batches of the problem\n",
    "- A minibatch is a random subset of the training data\n",
    "- We have many local improvements of the loss function instead of a slow global improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to select a random subset of indices of element in the training sample\n",
    "torch.randint(0, X.shape[0], (32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put all together\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((27, 2), generator=g, device=device)\n",
    "W1 = torch.randn((6, 100), generator=g, device=device)\n",
    "b1 = torch.randn(100, generator=g, device=device)\n",
    "W2 = torch.randn([100, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for epoch in range(10000):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "\n",
    "    if epoch < 10 or epoch % 1000 == 0:\n",
    "        print(f\"Epoch:{epoch}, loss={loss.item()}\")\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= 0.1 * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loss is evaluated only on the minibatches, not the whole training set\n",
    "- The quality of the gradient is lower\n",
    "- The loss is moving up and down, depending on the particular minibatch, but the trend is going down\n",
    "\n",
    "Usually, it is better to have multiple low-quality steps that a better slow high quality step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets evaluate the loss in the whole training set\n",
    "with torch.no_grad():\n",
    "    emb = C[X]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets play with different learning rates\n",
    "\n",
    "for learning_rate in [0.0001, 0.001, 0.01, 1, 10, 0.1]:\n",
    "    print(\"Learning rate:\", learning_rate)\n",
    "    print(\"=\"*30)\n",
    "    g = torch.Generator(device=device).manual_seed(31416)\n",
    "    C = torch.randn((27, 2), generator=g, device=device)\n",
    "    W1 = torch.randn((6, 100), generator=g, device=device)\n",
    "    b1 = torch.randn(100, generator=g, device=device)\n",
    "    W2 = torch.randn([100, len(all_chars)], generator=g, device=device)\n",
    "    b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    batch_size = 32\n",
    "\n",
    "    for epoch in range(1000):\n",
    "\n",
    "        # build minibatch\n",
    "        ix = torch.randint(0, X.shape[0], (batch_size,))\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[X[ix]]\n",
    "        h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Y[ix])\n",
    "\n",
    "        if epoch < 10 or epoch % 100 == 0:\n",
    "            print(f\"Epoch:{epoch}, loss={loss.item()}\")\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        for p in parameters:\n",
    "            p.data -= learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.1 seems to be the best value. Lets train the whole network with that value, and show the evolution of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put all together\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((27, 2), generator=g, device=device)\n",
    "W1 = torch.randn((6, 100), generator=g, device=device)\n",
    "b1 = torch.randn(100, generator=g, device=device)\n",
    "W2 = torch.randn([100, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "batch_size = 32\n",
    "\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20000):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= 0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets evaluate the loss in the whole training set\n",
    "with torch.no_grad():\n",
    "    emb = C[X]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the learning rate should be adjusted while training, going from larger values in the beginning to smaller values in final epochs. This is known as **learning rate decay**\n",
    "\n",
    "We will implement a very simple type of decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put all together\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((27, 2), generator=g, device=device)\n",
    "W1 = torch.randn((6, 100), generator=g, device=device)\n",
    "b1 = torch.randn(100, generator=g, device=device)\n",
    "W2 = torch.randn([100, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "batch_size = 32\n",
    "\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, X.shape[0], (batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[X[ix]]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix])\n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    learning_rate = 0.1 if epoch < num_epochs / 2 else 0.01\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets evaluate the loss in the whole training set\n",
    "with torch.no_grad():\n",
    "    emb = C[X]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this loss is lower than the best one we achieved with the bi-gram model: 2.47446\n",
    "\n",
    "But there is a problem: This model have much more parameters than the previous one, so maybe it is only overfitting by memorizing the training set.\n",
    "\n",
    "Solution: Split the available data intro three different sets:\n",
    "- Training set: used to tune parameters of the models\n",
    "- Validation set: used to select between models and to tune the hyper-parameters of a given model\n",
    "- Test set: used to estimate the power of generalization of a resultant model.\n",
    "\n",
    "Note:\n",
    "- Training and validation sets can be used freely, but the use of test set must be strictly limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X, device=device)\n",
    "    Y = torch.tensor(Y, device=device)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(314)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " len(words), len(Xtr), len(Xval), len(Xte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets transform all the previous code to the new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((27, 2), generator=g, device=device)\n",
    "W1 = torch.randn((6, 100), generator=g, device=device)\n",
    "b1 = torch.randn(100, generator=g, device=device)\n",
    "W2 = torch.randn([100, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "batch_size = 32\n",
    "\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20000):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    learning_rate = 0.1 if epoch < num_epochs / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr)\n",
    "    print(\"Training loss\", loss.item())\n",
    "\n",
    "    emb = C[Xval]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yval)\n",
    "    print(\"Validation loss\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both losses are similar, the network is not memorizing the data but actually learning from data. \n",
    "\n",
    "On the other hand, the model is underfitting, because both losses are quite similar. We can change to a model with more parameters.\n",
    "\n",
    "Lets increase the size of the hiden layer. \n",
    "- Note that we are here tunning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((27, 2), generator=g, device=device)\n",
    "W1 = torch.randn((6, 300), generator=g, device=device)\n",
    "b1 = torch.randn(300, generator=g, device=device)\n",
    "W2 = torch.randn([300, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "batch_size = 32\n",
    "\n",
    "lossi = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(20000):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    lossi.append(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    learning_rate = 0.1 if epoch < num_epochs / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr)\n",
    "    print(\"Training loss\", loss.item())\n",
    "\n",
    "    emb = C[Xval]\n",
    "    h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yval)\n",
    "    print(\"Validation loss\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the loss is lower, and we can keep increasing the number of network parameters\n",
    "- Increasing the number of parameters usually increase the training time.\n",
    "\n",
    "Lets take a look to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of variance in the loss due to the batch size, which is a non-representative percentage of the whole dataset. Lets increase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner():\n",
    "    g = torch.Generator(device=device).manual_seed(31416)\n",
    "    C = torch.randn((27, 2), generator=g, device=device)\n",
    "    W1 = torch.randn((6, 300), generator=g, device=device)\n",
    "    b1 = torch.randn(300, generator=g, device=device)\n",
    "    W2 = torch.randn([300, len(all_chars)], generator=g, device=device)\n",
    "    b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "    batch_size = 64\n",
    "\n",
    "    lossi = []\n",
    "    \n",
    "    for epoch in range(20000):\n",
    "\n",
    "        # build minibatch\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[Xtr[ix]]\n",
    "        h = torch.tanh((emb.view(-1, 6) @ W1 + b1))\n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits, Ytr[ix])\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "        lossi.append(loss.item())\n",
    "\n",
    "        # backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        learning_rate = 0.1 if epoch < num_epochs / 2 else 0.01\n",
    "        # update\n",
    "        for p in parameters:\n",
    "            p.data -= learning_rate * p.grad\n",
    "\n",
    "    plt.plot(epochs, lossi)\n",
    "\n",
    "\n",
    "inner()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now the variance is lower, but the memory and training time increases: we have a tradeoff here.\n",
    "\n",
    "Now, lets visualize the embedding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(C[:,0].data.cpu(), C[:,1].data.cpu(), s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha='center', va='center', color='white')\n",
    "plt.grid('minor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We significantly improve the number of neurons in the hidden layer, but the impact in loss is not significant. Lets consider a larger embedding dimmension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 10\n",
    "hidden_layer_size = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((27, embedding_size), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*embedding_size, hidden_layer_size), generator=g, device=device)\n",
    "b1 = torch.randn(hidden_layer_size, generator=g, device=device)\n",
    "W2 = torch.randn([hidden_layer_size, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "batch_size = 32\n",
    "\n",
    "epochs = []\n",
    "lossi = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20000):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]]\n",
    "    h = torch.tanh((emb.view(-1, block_size*embedding_size) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    \n",
    "    epochs.append(epoch)\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    learning_rate = 0.1 if epoch < num_epochs / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[Xtr]\n",
    "    h = torch.tanh((emb.view(-1, block_size*embedding_size) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr)\n",
    "    print(\"Training loss\", loss.item())\n",
    "\n",
    "    emb = C[Xval]\n",
    "    h = torch.tanh((emb.view(-1, block_size*embedding_size) @ W1 + b1))\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yval)\n",
    "    print(\"Validation loss\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is getting better, and some overfit starts to appear.\n",
    "\n",
    "You can tune many network parameters in order to decrease the loss:\n",
    "- Add neurons to the hidden layer\n",
    "- Use a larger block size (more letters)\n",
    "- Increase the number of layers\n",
    "\n",
    "Finally, when the quality cannot be improved any longer, use the test set to have an accurate estimation about the quality of the model while dealing with unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New names generated\n",
    "\n",
    "for _ in range(20):\n",
    "    xs = [0, 0, 0]\n",
    "    letters = []\n",
    "    while True:\n",
    "        emb = C[torch.tensor(xs)]\n",
    "        h = torch.tanh((emb.view(-1) @ W1 + b1))\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        ix = torch.multinomial(probs,  num_samples=1, replacement=True).item()\n",
    "        letters.append(itos[ix])\n",
    "        xs = xs[1:] + [ix]\n",
    "        if ix == 0:        \n",
    "            break\n",
    "    print(''.join(letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting modification is keeping only the largest probabilities while creating the new names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "for _ in range(20):\n",
    "    xs = [0, 0, 0]\n",
    "    letters = []\n",
    "    while True:\n",
    "        emb = C[torch.tensor(xs)]\n",
    "        h = torch.tanh((emb.view(-1) @ W1 + b1))\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "\n",
    "        _, indices = torch.topk(probs, k)\n",
    "        mask = torch.zeros_like(probs)\n",
    "        mask.scatter_(0, indices, 1)\n",
    "        probs = probs * mask\n",
    "\n",
    "        ix = torch.multinomial(probs,  num_samples=1, replacement=True).item()\n",
    "        letters.append(itos[ix])\n",
    "        xs = xs[1:] + [ix]\n",
    "        if ix == 0:        \n",
    "            break\n",
    "    print(''.join(letters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
