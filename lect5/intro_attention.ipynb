{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "all_chars = ['.'] + sorted(list(set(\"\".join(words))))\n",
    "itos = {idx: v for idx, v in enumerate(all_chars)}\n",
    "stoi = {v: k for k, v in itos.items()}\n",
    "\n",
    "NUM_CHARS = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(314)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.7 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xte, Yte = build_dataset(words[n1:])\n",
    "\n",
    "print(Xtr.shape, Ytr.shape)\n",
    "print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 10\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding = nn.Linear(NUM_CHARS, EMBED_SIZE, bias=False)\n",
    "        self.layer1 = nn.Linear(3 * EMBED_SIZE, 32)\n",
    "        self.layer2 = nn.Linear(32, NUM_CHARS)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = F.one_hot(xs, num_classes=NUM_CHARS).to(torch.float)\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(xs.shape[0], -1)\n",
    "        x = self.layer1(x).tanh()\n",
    "        x = self.layer2(x)\n",
    "        # x = self.softmax(x) ... CrossEntropyLoss already contains softmax\n",
    "        return x\n",
    "    \n",
    "model = Model()\n",
    "sum([n.nelement() for n in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = Xtr[:5]\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Linear(NUM_CHARS, EMBED_SIZE, bias=False)\n",
    "embedded = embedding(F.one_hot(xs, num_classes=NUM_CHARS).to(torch.float))\n",
    "embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the quality of generated model by communicating the generated characters, in order to improve the quality of next generated character. \n",
    "\n",
    "A mechanism for such communication is Attention:\n",
    "- Each character (represented by the embedding) is represented as a Value\n",
    "- Each character have an associated Key, that describes what it can provide\n",
    "- Each character have an associated Query, that describes the information it needs\n",
    "\n",
    "All this transformations are learned while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEAD_SIZE = 8\n",
    "Q = nn.Linear(EMBED_SIZE, HEAD_SIZE)\n",
    "K = nn.Linear(EMBED_SIZE, HEAD_SIZE)\n",
    "V = nn.Linear(EMBED_SIZE, HEAD_SIZE)\n",
    "\n",
    "item = embedding(F.one_hot(Xtr[:10], num_classes=NUM_CHARS).to(torch.float))\n",
    "q = Q(item)\n",
    "k = K(item)\n",
    "v = V(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item.shape, q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The query is multiplied by the key, in order to build the attention matrix\n",
    "k.transpose(-2, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = (q @ k.transpose(-2, -1))\n",
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = F.softmax(attentions, dim=-1)\n",
    "probs = weights @ v\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for each element in the batch, and for each character, we have a representation that now includes its relations to other characters ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.embedding = nn.Linear(NUM_CHARS, EMBED_SIZE, bias=False)\n",
    "        self.layer1 = nn.Linear(3 * HEAD_SIZE, 24)\n",
    "        self.layer2 = nn.Linear(24, NUM_CHARS)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.Q = nn.Linear(EMBED_SIZE, HEAD_SIZE)\n",
    "        self.K = nn.Linear(EMBED_SIZE, HEAD_SIZE)\n",
    "        self.V = nn.Linear(EMBED_SIZE, HEAD_SIZE)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = F.one_hot(xs, num_classes=NUM_CHARS).to(torch.float)\n",
    "        x = self.embedding(x)\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        attentions = (q @ k.transpose(-2, -1))\n",
    "        weights = F.softmax(attentions, dim=-1)\n",
    "        probs = weights @ v        \n",
    "        x = probs.view(xs.shape[0], -1)\n",
    "        x = self.layer1(x).tanh()\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    \n",
    "model2 = Model2()\n",
    "sum([n.nelement() for n in model2.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = Xtr[:5]\n",
    "model2(xs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "Xtr_dev = Xtr.to(device)\n",
    "Ytr_dev = Ytr.to(device)\n",
    "model2_dev = model2.to(device)\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model2_dev.train()\n",
    "\n",
    "    outputs = model2_dev(Xtr_dev)\n",
    "    loss = loss_fn(outputs, Ytr_dev)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(epoch, loss.item())\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
