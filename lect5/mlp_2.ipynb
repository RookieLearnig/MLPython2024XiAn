{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chars = ['.'] + sorted(list(set(\"\".join(words))))\n",
    "itos = {idx: v for idx, v in enumerate(all_chars)}\n",
    "stoi = {v: k for k, v in itos.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X, device=device)\n",
    "    Y = torch.tensor(Y, device=device)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(314)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "print(Xtr.shape, Ytr.shape)\n",
    "print(Xval.shape, Yval.shape)\n",
    "print(Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(lossi).view((-1, 1000)).mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    \"train\": (Xtr, Ytr),\n",
    "    \"val\": (Xval, Yval),\n",
    "    \"test\": (Xte, Yte)\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x, y = splits[split]\n",
    "    emb = C[x] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample of the model\n",
    "for _ in range(20):\n",
    "    xs = [0, 0, 0]\n",
    "    letters = []\n",
    "    while True:\n",
    "        emb = C[torch.tensor(xs)]\n",
    "        h = torch.tanh((emb.view(-1) @ W1 + b1))\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        ix = torch.multinomial(probs,  num_samples=1, replacement=True).item()\n",
    "        letters.append(itos[ix])\n",
    "        xs = xs[1:] + [ix]\n",
    "        if ix == 0:        \n",
    "            break\n",
    "    print(''.join(letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving initializations\n",
    "\n",
    "The large drop in the quality from the first iterations to the remaining is due to weights initialization.\n",
    "\n",
    "By the way, due to no prior knowledge exists about character probabilities, we could expect they are equiprobable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.tensor(1 / vocab_size).log().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is quite lower than those obtained in the first network iterations, so the network weights contains 'crazy' distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D example of the issue\n",
    "logits = torch.tensor([0.0, 0.0, 0.0, 0.0])\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[2].log()  # any index ....\n",
    "probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A high value, selected\n",
    "logits = torch.tensor([0.0, 0.0, 5.0, 0.0])\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[2].log()  \n",
    "probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A high value, unselected\n",
    "logits = torch.tensor([0.0, 0.0, 5.0, 0.0])\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[1].log()  \n",
    "probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random one, the loss is constrained\n",
    "logits = torch.randn(4)\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[1].log()  \n",
    "logits, probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A random one, big values, loss could explode, and is unstable ...\n",
    "logits = torch.randn(4) * 10\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[1].log()  \n",
    "logits, probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even bigger ...\n",
    "logits = torch.randn(4) * 100\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "loss = -probs[1].log()  \n",
    "logits, probs, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In initialization, we might want uniformly distributed values\n",
    "- Smaller values in the matrix allows small losses in untrained matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    break\n",
    "\n",
    "print(logits[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits contains really large values, what creates the very large loss. To solve this we will:\n",
    "- Set all biases to zero\n",
    "- Set all weights to small values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    break\n",
    "\n",
    "print(logits[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As desired, the loss is now closer to the theoretical expected value: 3.\n",
    "\n",
    "Lets run some iterations with the new initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 1000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % (max_steps // 10) == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_loss('train')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, now we have an smoother loss function even in the beginning. \n",
    "\n",
    "- Initialization problems can make to waste the initial epochs, instead of using the effort to later loss improvements.\n",
    "\n",
    "- Now, both loss values are smaller than with the original initialization (2.22, 2.25)\n",
    "\n",
    "## Second problem, arguments to activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the values returned by the activation function\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how many values are very close to 1 and -1. In all this cases, the neuron is in a zone that cannot learn (remember, gradient is very close to zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tanh():\n",
    "    x = torch.arange(-5, 5, 0.1)\n",
    "    y = x.tanh()\n",
    "    der_y = 1 - torch.tanh(x) ** 2\n",
    "    plt.plot(x, y)\n",
    "    plt.plot(x, der_y)\n",
    "    plt.show()\n",
    "\n",
    "show_tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets calculate the histogram of h values\n",
    "plt.hist(h.view(-1).tolist(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see that most values are located in zones where the tanh function cannot learn.\n",
    "# lets see the pre-activation values\n",
    "plt.hist(hpreact.view(-1).tolist(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see it is very broad, and that explains the tanh results\n",
    "# lets see the distribution of h values per training instance\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(h.cpu().abs()>0.99, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the image is full of white dots, spread across all the neurons and instances.\n",
    "- there could be cases where a whole columns is white, so the neuron is completelly dead\n",
    "- the same behavior can be found in other activation functions like sigmoids and ReLU\n",
    "\n",
    "The solution to this problem is transforming the values of hpreact close to zero, by modifying the weight and bias of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)    * 0.1\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)                         * 0.0\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist(), bins=50)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(h.abs().cpu()>0.99, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using other values for initialization generates other distributions of h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)    * 0.2\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)                         * 0.0\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist(), bins=50)\n",
    "plt.show()\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(h.abs().cpu()>0.99, cmap='gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the whole training\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)    * 0.2\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)                         * 0.0\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 20000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % (max_steps // 10) == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_loss('train')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a new improvement to the loss function (2.0985, 2.1490). This is because all the neurons are working most of the time in the training process.\n",
    "\n",
    "**Loss log**:\n",
    "- original: \n",
    "\n",
    "train 2.1261403560638428\n",
    "\n",
    "test 2.184201955795288\n",
    "\n",
    "- fix softmax confidently wrong:\n",
    "\n",
    "train 2.098554849624634\n",
    "\n",
    "test 2.151556968688965\n",
    "\n",
    "- fix tanh layer too saturated at init\n",
    "\n",
    "train 2.064349412918091\n",
    "\n",
    "test 2.120842933654785\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is shallow, even with wrong initializations the results are quite good. In deeper networks errors are stacked and can turn the network unusable, or very hard to train.\n",
    "\n",
    "In general, it is quite hard to assign the reducing factors by hand, and some rules can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200)\n",
    "y = x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True)\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that both are Gaussian with mean zero, but the standar deviation is larger in the product, so values are more spread around the curve.\n",
    "- This is why we saw in the network that the activation values are so frequently larger\n",
    "\n",
    "**Question**: How to scale the weights for achieving a better behavior\n",
    "\n",
    "Intuition: reduce the weight values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) * 0.2\n",
    "y = x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std()) \n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True)\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, what value to use to obtaing a standard deviation of 1?\n",
    "- From statistics: divide the weights for the square root of the number of rows (fan in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) / 10**0.5\n",
    "y = x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std()) \n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True)\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In very deep networks we need that this activations do not grow or shrink too much.\n",
    "\n",
    "When we have activation functions, we need to also take care of them when initializing. Lets see the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) / 10**0.5\n",
    "y = (x @ w).tanh()\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std()) \n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1).tolist(), 50, density=True)\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1).tolist(), 50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this [paper](papers/He_Delving_Deep_into_ICCV_2015_paper.pdf), the problem is explored and solutions are proposed. They found that, to compensate for squashing functions, we need to use a factor. \n",
    "\n",
    "Torch has a function for performing that correction, named *torch.nn.init.kaiming_normal*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past, tunning all this parameters were crucial, and the resulting network was frequetly fragile. \n",
    "- Fortunatelly, we have now other tools that makes the final quality less sensitive to weight initialization\n",
    "\n",
    "One of these tools is batch normalization.\n",
    "\n",
    "## Batch Normalization\n",
    "\n",
    "Batch normalization is introduced in 2015 for a team at Google as a way to allow training very deep networks [PDF](papers/ioffe15.pdf)\n",
    "\n",
    "The proposed idea is, instead of trying to guarantee the good properties of the pre-activation values by initializing the weights, lets directly modify the matrixes before being used on each epoch.\n",
    "\n",
    "- It can be done because normalizing the matrix is a differentiable operation, so it can be included in the backpropagation mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add this to our current implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the whole training\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416+1)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)    * 0.2\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)                         * 0.0\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "\n",
    "    # doing this, every single neuron has mean 0 and stdev 1 for all its activations in the batch\n",
    "    hpreact = (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True)\n",
    "\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with this normalization is that it completelly removes the bias and any multiplicative factor of the weight matrix, so we need to add them manually after normalization.º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the whole training\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416+1)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)    * 0.2\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)                         * 0.0\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden), device=device)\n",
    "bnbias = torch.zeros((1, n_hidden), device=device)\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "max_steps = 1000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "\n",
    "    # doing this, every single neuron has mean 0 and stdev 1 for all its activations in the batch\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % (max_steps // 10) == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss2(split):\n",
    "    x, y = splits[split]\n",
    "    emb = C[x] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss2('train')\n",
    "split_loss2('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the evaluation code we are estimating the mean and deviation using the whole validation data but, what happend if we need to process a single instance?\n",
    "- This shows an important problem of batch normalization: we are tighting together many examples randomly taken for guiding the learning process. There are other normalization methods that solves this issue.\n",
    "- Other problem is what values of mean and deviation to use in the exploitation of the network.\n",
    "\n",
    "The solution proposed in the paper for this last problem is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the whole training\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "g = torch.Generator(device=device).manual_seed(31416+1)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g, device=device)\n",
    "W1 = torch.randn((block_size*n_embd, n_hidden), generator=g, device=device)    * 0.2\n",
    "b1 = torch.randn(n_hidden, generator=g, device=device)                         * 0.0\n",
    "W2 = torch.randn([n_hidden, len(all_chars)], generator=g, device=device)       * 0.01\n",
    "b2 = torch.rand(len(all_chars), generator=g, device=device)                    * 0.0\n",
    "\n",
    "bngain = torch.ones((1, n_hidden), device=device)\n",
    "bnbias = torch.zeros((1, n_hidden), device=device)\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "\n",
    "print(\"# Params: \", sum(p.nelement() for p in parameters))\n",
    " \n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "bnmean_running = torch.zeros((1, n_hidden), device=device)\n",
    "bnstd_running = torch.ones((1, n_hidden), device=device)\n",
    "\n",
    "max_steps = 100000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(max_steps):\n",
    "\n",
    "    # build minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnstd = hpreact.std(0, keepdim=True)\n",
    "    hpreact = bngain * (hpreact - bnmean) / bnstd + bnbias\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = 0.99 * bnmean_running + 0.01 * bnmean\n",
    "        bnstd_running = 0.99 * bnstd_running + 0.01 * bnstd\n",
    "\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward() \n",
    "\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"{epoch:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    learning_rate = 0.1 if epoch < max_steps / 2 else 0.01\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= learning_rate * p.grad   \n",
    "\n",
    "    break   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnmean_running.shape, bnstd_running.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss2(split):\n",
    "    x, y = splits[split]\n",
    "    emb = C[x] \n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss2('train')\n",
    "split_loss2('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although its drawbacks, using batch statistics in batch normalization has a regularization effect that additionally improves the results:\n",
    "- The slight modification of all the activation matrixes works as a type of data augmentation \n",
    "\n",
    "## Batch normalization using torch.nn\n",
    "\n",
    "[nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(31416)\n",
    "\n",
    "# Define the custom model class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_embd, n_hidden):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, n_embd, device=device)\n",
    "        self.flat = nn.Flatten(start_dim=1)\n",
    "        self.h_layer = nn.Linear(block_size*n_embd, n_hidden, bias=False, device=device)\n",
    "        self.bn_layer = nn.BatchNorm1d(num_features=n_hidden, device=device)\n",
    "        self.out_layer = nn.Linear(n_hidden, len(all_chars), device=device)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        x = self.embedding_layer(xs).flatten(1)\n",
    "        x = self.h_layer(x)\n",
    "        x = self.bn_layer(x).tanh()\n",
    "        x = self.out_layer(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel(n_embd = 10, n_hidden = 200)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "max_steps = 10000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(max_steps):\n",
    "    # Forward pass\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "    outputs = model(Xb)\n",
    "    loss = F.cross_entropy(outputs, Yb)\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={loss.item()}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "        \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    \"train\": (Xtr, Ytr),\n",
    "    \"val\": (Xval, Yval),\n",
    "    \"test\": (Xte, Yte)\n",
    "}\n",
    "\n",
    "@torch.no_grad()\n",
    "def split_loss2(split):\n",
    "    x, y = splits[split]\n",
    "    outputs = model(x)\n",
    "    loss = F.cross_entropy(outputs, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "model.eval()  # Turn to eval mode, for batch normalization to work ok\n",
    "split_loss2('train')\n",
    "split_loss2('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = torch.tensor(lossi).view(-1, 1000).mean(dim=1)\n",
    "plt.plot(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
