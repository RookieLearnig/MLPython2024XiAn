{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generating names by a training a neural networks using bigrams\n",
    "\n",
    "In this first solution, we train the model by counting the bigrams appearance. A different approach is to train a neural network in order to predict the next character.\n",
    "- The network is fed with the current character, and returns an estimation of the probabilities of the next character\n",
    "- Since we have a loss function, we will be able to evaluate the behavior of different configurations of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = open('data/names.txt', 'r').read().splitlines()\n",
    "all_chars = ['.'] + sorted(list(set(\"\".join(words))))\n",
    "itos = {idx: v for idx, v in enumerate(all_chars)}\n",
    "stoi = {v: k for k, v in itos.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets create the training sample\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = ch1, ch2\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:10], ys[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to feed the input layer of the network, we need to encode the characters\n",
    "# We will use one-hot encoding\n",
    "\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs[:5], num_classes=len(all_chars))\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to change the data type to float in order to feed the network\n",
    "xenc = F.one_hot(xs[:5], num_classes=len(all_chars)).float()\n",
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a neuron layer with 27 neurons with no bias and no activation function.\n",
    "- We use 27 neurons in order to return the probability of each of the 27 posible output characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27, 27))\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can understand the 27 outputs of each input like a quantity directly correlated with how much the output character comes after the input character.\n",
    "\n",
    "Since we have here positive and negative numbers, we will transform them by the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc @ W).exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all results are positive, and since the exponential is monotonic, it preserves the order of the results. This values can be understood as the \"counts\".\n",
    "\n",
    "To turn the values to probabilities, we perform the same transformation we did with the counts in the original solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = (xenc @ W) # log-counts \n",
    "counts = logits.exp() # equivalent to the M metric\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**probs** contains the probability of generating the character in the columns (from the 27 candidates) when the input is each of the first 5 characters.\n",
    "\n",
    "Note: All this operations are differentiable, so we can backpropagate.\n",
    "\n",
    "We only need a loss. Lets check the current result on the first character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[0].item(), probs[0][ys[0].item()].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the results are now far from the expected, we need to find (using backpropagation and gradient descent) good values for W, so the largest probabilities are assigned to the correct characters in the sequences.\n",
    "\n",
    "Lets make an small summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(itos[c1.item()], itos[c2.item()]) for c1, c2 in zip(xs[:5], ys[:5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialize 27 neurons, each one receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.rand((27, 27), generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs[:5], num_classes=len(all_chars)).float()\n",
    "logits = (xenc @ W) # log-counts \n",
    "counts = logits.exp() # equivalent to the M metric\n",
    "probs = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last two lines is named \"softmax\", that transforms a layer outputs into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(probs, logits.softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(xs[:5], num_classes=len(all_chars)).float()\n",
    "logits = (xenc @ W) # log-counts \n",
    "probs = logits.softmax(dim=1)\n",
    "\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    p = probs[i, y]  # probability assigned by the network to real output\n",
    "    logp = torch.log(p)\n",
    "    nll = -logp\n",
    "    nlls[i] = nll \n",
    "print(nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play with different randomly generated Ws.\n",
    "\n",
    "Now, it is time to train the neural network, but first we need to define the loss function. We will us the same meassure: the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0, 5], probs[1, 13], probs[2, 13], probs[3, 1], probs[4, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a way of selecting elements in pytorch. If we index a tensor using arrays of tensors, they are used as indexes in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[torch.arange(5), ys[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative log-likehood\n",
    "-probs[torch.arange(5), ys[:5]].log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we need a single number, we will use the mean\n",
    "loss = -probs[torch.arange(5), ys[:5]].log().mean()\n",
    "loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss, together with soft max, is combined in torch with in CrossEntropyLoss.\n",
    "\n",
    "Now we have all the ingredients to train the neural network to minimize the loss function. \n",
    "\n",
    "Lets put the training code in torch, which will be pretty similar to the one in micrograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "W = torch.rand((27, 27), requires_grad=True)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam([W], lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    xenc = F.one_hot(xs, num_classes=len(all_chars)).float()\n",
    "    logits = (xenc @ W) # log-counts \n",
    "    loss = loss_fn(logits, ys)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "        \n",
    "    # update\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % (num_epochs // 10) == 0:\n",
    "        print(loss)\n",
    "\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The bigram based model achieved a loss of 2.4541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It happens that our network is slowly converging to the model we creating by using the counts, which is optimal for all models using bigrams.\n",
    "\n",
    "Lets generate some names with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    ix = 0\n",
    "    result = []\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=len(all_chars)).float()\n",
    "        logits = (xenc @ W) # log-counts \n",
    "        p = logits.softmax(dim=1)\n",
    "        \n",
    "        ix = torch.multinomial(p,  num_samples=1, replacement=True, generator=g)\n",
    "        ix = ix.item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        result.append(itos[ix])\n",
    "    print(\"\".join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_teach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
