{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174f71ee",
   "metadata": {},
   "source": [
    "# Introduction to Backpropagation and Neural Networks with AutoGrad\n",
    "Following the video and code of Andrej Karpathy\n",
    "https://www.youtube.com/watch?v=VMj-3S1tku0&t=1072s&pp=ugMICgJlcxABGAHKBRJtaWNyb2dyYWQga2FycGF0aHk%3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b075ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2636d2",
   "metadata": {},
   "source": [
    "Lets start by defining a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9366cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df3e1a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We can, of course, evaluate the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ee98c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f(3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3498362",
   "metadata": {},
   "source": [
    "Lets plot this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc59a19",
   "metadata": {},
   "source": [
    "What is derivative meassuring?\n",
    "\n",
    "df/dx = lim h->0 (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478daaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "x = 3.0\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df46592",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aded0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52650ae9",
   "metadata": {},
   "source": [
    "Solving analytically we find dy/dx = 6*x-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "6 * x - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607a11dc",
   "metadata": {},
   "source": [
    "The derivative in the point is the slope, or instaneous increment of the function when the argument incresase:\n",
    "- since derivative is positive the funcion is increasing in that point\n",
    "- the increase is proportional to 14 times the increment in the function parameter\n",
    "\n",
    "Lets try a different value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05739504",
   "metadata": {},
   "source": [
    "Now the function is also increasing, but now slower\n",
    "\n",
    "Lets check another value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -2\n",
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7862d",
   "metadata": {},
   "source": [
    "Note than in x=2 the function is decreasing, faster ...\n",
    "\n",
    "Lets see another point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067a87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2/3\n",
    "h = 0.0001\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e97b50",
   "metadata": {},
   "source": [
    "It is very close to 0, so the function is neither increasing nor decreasing at that point\n",
    "\n",
    "A more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eea035",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "f = a*b+c\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e2f75",
   "metadata": {},
   "source": [
    "Let see how the function changes with respect to the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f10622",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "df_da = (((a+h)*b + c) - (a*b + c)) / h\n",
    "df_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430a261",
   "metadata": {},
   "source": [
    "This -3 means that the function value decreases proportional to 3 times the increment in 'a'. \n",
    "\n",
    "Lets see some other increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b3cd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db = ((a*(b+h) + c) - (a*b + c)) / h\n",
    "df_dc = ((a*b+c+h) - (a*b+c)) / h\n",
    "\n",
    "df_da, df_db, df_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1861539",
   "metadata": {},
   "source": [
    "You can check the values are very close to the analytical \"partial derivatives\"\n",
    "How can I modify the parameters a, b, and c if I want to increase the value of f?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c14a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.01\n",
    "a*b+c, (a-h)*(b+h)+(c+h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365d0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = -0.01\n",
    "a*b+c, (a-h)*(b+h)+(c+h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926fff1",
   "metadata": {},
   "source": [
    "# Gradient descend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b5e86e",
   "metadata": {},
   "source": [
    "Lets create a class for storing values, adding operations and parameters of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "a = Value(2)\n",
    "b = Value(-3.0)\n",
    "d = (a*b)\n",
    "d, d._prev, d._op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf74a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let create a way to visualize the expression\n",
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{data: %s}\" % (n.data), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a699b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = Value(2)\n",
    "b = Value(-3.0)\n",
    "c = Value(10)\n",
    "d = a*b + c\n",
    "\n",
    "draw_dot(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852e0e0",
   "metadata": {},
   "source": [
    "Lets add some labels to the nodes, in order to identify them with ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718249a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{%s|data: %s}\" % (n.label, n.data), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label='d'\n",
    "f = Value(-2, label='f')\n",
    "L = d*f; L.label='L'\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1190b2",
   "metadata": {},
   "source": [
    "You can see that we have a matematical expression linking L with four free parameters: a, b, c, and f. We are now to run backpropagation, trying to increase the value of L by changing the values of the free parameters.\n",
    "- For every single value we are going to calculate the derivative, using the chain rule. Using this, we will know how to change the values for increasing L \n",
    "\n",
    "In order to do so, we will add a property in Value to hold the derivative of L with respect to that value. We will name this property 'grad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca487d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{%s|data: %.4f|grad:%.4f}\" % (n.label, n.data, n.grad), \n",
    "             shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n",
    "\n",
    "a = Value(2, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10, label='c')\n",
    "e = a*b; e.label='e'\n",
    "d = e + c; d.label='d'\n",
    "f = Value(-2, label='f')\n",
    "L = d*f; L.label='L'\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7d50ba",
   "metadata": {},
   "source": [
    "We start back to front, manually. We started by L\n",
    "\n",
    "dL/dL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7abed6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "L.grad = 1\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28c6d4",
   "metadata": {},
   "source": [
    " Now, lets calculate the derivatives with respect to f and d. Since L = f*d:\n",
    " \n",
    " dL/df = d\n",
    " \n",
    " dL/dd = f\n",
    " \n",
    " Lets check by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2+h, label='f')  # HERE    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8d2961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    d.data += h                 # HERE\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dfa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since they are correct, we will set the values\n",
    "d.grad = f.data\n",
    "f.grad = d.data\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a97dd",
   "metadata": {},
   "source": [
    "Now, lets move to the previous values, e and c. We now need to calculate dL/de and dL/dc. From the chain rule we know that:\n",
    "\n",
    "dL/de = dL/dd*dd/de\n",
    "- We already calculated dL/dd, which is -2\n",
    "- Since d = e+c, dd/de=1\n",
    "- As a result **dL/de = -2 * 1 = -2**\n",
    "\n",
    "In a similar waw, **dL/dc = -2**\n",
    "\n",
    "**Note**: The + node only passes the gradient without modification\n",
    "\n",
    "Lets check numerically if it is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436451f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    e.data += h              # HERE\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792df90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10+h, label='c') # HERE\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167c2953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# After the check we know it is correct, so we update the .grad property\n",
    "e.grad = -2\n",
    "c.grad = -2\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e7ad2",
   "metadata": {},
   "source": [
    "Now, lets calculate the final one, the gradient for *a* and *b*, also using the chain rule.\n",
    "\n",
    "- dL/da = dL/de * de/da\n",
    "- dL/de is already known, -2\n",
    "- Since e = a*b, then de/da = b\n",
    "\n",
    "Finally, dL/da = -2 * -3 = 6\n",
    "\n",
    "Similarly, dL/db = dL/de*de/db = -2 * a = -2 * 2 = -4\n",
    "\n",
    "Lets check it numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdf942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2+h, label='a') # HERE\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14893d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_hand():\n",
    "\n",
    "    h = 0.01\n",
    "\n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0+h, label='b') # HERE\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'    \n",
    "    f = Value(-2, label='f')    \n",
    "    L = d*f; L.label='L'   \n",
    "    L2 = L.data\n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "grad_by_hand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df8508",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since they are correct, we updated the .grad properties.\n",
    "a.grad = 6\n",
    "b.grad = -4\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557fba6",
   "metadata": {},
   "source": [
    "**Note**: There are some parameters that we can change, like a, b, c, and f, while the others are calculated, so cannot be changed.\n",
    "\n",
    "Now, let use the gradient in order to increase the value of L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0557adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag, bg, cg, fg = a.grad, b.grad, c.grad, f.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_by_hand(h = 0.01):\n",
    "    \n",
    "    a = Value(2, label='a')\n",
    "    b = Value(-3.0, label='b')\n",
    "    c = Value(10, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L1 = L.data\n",
    "    \n",
    "    a = Value(2 + ag * h, label='a')\n",
    "    b = Value(-3.0 + bg * h, label='b')\n",
    "    c = Value(10 + cg * h, label='c')\n",
    "    e = a*b; e.label='e'\n",
    "    d = e + c; d.label='d'\n",
    "    f = Value(-2 + fg * h, label='f')\n",
    "    L = d*f; L.label='L'\n",
    "    L2 = L.data\n",
    "\n",
    "    print(L1, L2, L2-L1)\n",
    "    \n",
    "eval_by_hand(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677eff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_by_hand(-0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af34270",
   "metadata": {},
   "source": [
    "Note, it is increased. This is the backpropagation algorithm in action! \n",
    "\n",
    "Lets move to something more complex, like a neuron.\n",
    "\n",
    "A neural network has:\n",
    "- Neurons\n",
    "    - Weights\n",
    "    - Bias (neuron default activation in absence of inputs)\n",
    "    - Activation function: \n",
    "        - Introduce nonlinearities, frequently squashing the neuron output\n",
    "        \n",
    "There are some common activation function, like the tanh and ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2437561",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "plt.plot(x, np.tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec11d7b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5, 5, 0.1)\n",
    "\n",
    "plt.plot(x, np.where(x > 0, x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c99b45",
   "metadata": {},
   "source": [
    "Lets create a sistem a neuron and two inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2969fd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.7, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "\n",
    "draw_dot(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a30b9",
   "metadata": {},
   "source": [
    "Lets add the code for the activation function. \n",
    "- Since in our Value we only sums and products, we need a new node for the tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1305076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        return out\n",
    "        \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144d5af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264ed602",
   "metadata": {},
   "source": [
    "Note than here the efect of tanh is minimal, but lets change the bias to 10 we would see the squashing efect of tanh. Now, lets run backpropagation on the neuron.\n",
    "\n",
    "**Note**. While training the neuron, the only parameters that we can change are the weights and biases, because the training examples are fixed.\n",
    "\n",
    "First, since do/do=1, lets set that in the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ff24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d428f",
   "metadata": {},
   "source": [
    "dtanh(x)/dx = 1 - tanh(x)^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225551f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.grad = 1 - (o.data)**2\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify():\n",
    "    h = 0.001\n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L1 = o.data\n",
    "    \n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    n.data += h\n",
    "    o = n.tanh(); \n",
    "    L2 = o.data\n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "verify()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd9c76",
   "metadata": {},
   "source": [
    "do/db = do/dn*dn/db = 0.5 * 1\n",
    "\n",
    "do/d(x1w1+x2w2) = do/dn * dn/d(x1w1+x2w2) = 0.5 * 1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f902d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b.grad = 0.5\n",
    "x1w1x2w2.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce22b8",
   "metadata": {},
   "source": [
    "do/dx2w2 = do/d(x1w1+x2w2) * d(x1w1+x2w2)/dx2w2\n",
    "    = 0.5 * 1 = 0.5\n",
    "    \n",
    "do/dx1w1 = do/d(x1w1+x2w2) * d(x1w1+x2w2)/dx1w1\n",
    "    = 0.5 * 1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb7d6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x2w2.grad = 0.5\n",
    "x1w1.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9efa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify():\n",
    "    h = 0.001\n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L1 = o.data\n",
    "    \n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x1w1.data += h\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b;\n",
    "    o = n.tanh(); \n",
    "    L2 = o.data\n",
    "    \n",
    "    print((L2-L1)/h)\n",
    "verify()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57ead2",
   "metadata": {},
   "source": [
    "do/dw1 = do/dx1w1 * dx1w1/dw1 = 0.5*x1 = 0.5*2 = 1\n",
    "\n",
    "do/dx1 = do/dx1w1 * dx1w1/dx1 = 0.5*w1 = 0.5*-3 = -1.5\n",
    "\n",
    "do/dw2 = do/dx2w2 * dx2w2/dw2 = 0.5*x2 = 0.5*0 = 0\n",
    "\n",
    "d0/dx2 = do/dx2w2 * dx2w2/dx2 = 0.5*w2 = 0.5*1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.grad = 1\n",
    "x1.grad = -1.5\n",
    "w2.grad = 0\n",
    "x2.grad = 0.5\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789dc939",
   "metadata": {},
   "source": [
    "Now, the parameters we can modify in order to increase the value of the function are the weights and biases.\n",
    "\n",
    "Lets modify them to icrease the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed59bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w1 = 1\n",
    "grad_w2 = 0\n",
    "grad_b = 0.5\n",
    "\n",
    "def modify(h):\n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0), Value(1.0)\n",
    "    b = Value(6.8812735870195432)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L1 = o.data\n",
    "    \n",
    "    x1, x2 = Value(2.0), Value(0.0)\n",
    "    w1, w2 = Value(-3.0 + h*grad_w1), Value(1.0+h*grad_w2)\n",
    "    b = Value(6.8812735870195432 + h*grad_b)\n",
    "    x1w1 = x1*w1\n",
    "    x2w2 = x2*w2\n",
    "    x1w1x2w2 = x1w1+x2w2\n",
    "    n = x1w1x2w2+b; \n",
    "    o = n.tanh(); \n",
    "    L2 = o.data\n",
    "    return L1, L2, L2-L1\n",
    "\n",
    "print(modify(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27da0c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(modify(-0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec758662",
   "metadata": {},
   "source": [
    "You can see that applying backpropagation is very simple, but tedious. Lets move to automatically calculate the gradient (autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = 1.0 * out.grad\n",
    "            other.grad = 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * other.data\n",
    "            other.grad = out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6737994",
   "metadata": {},
   "source": [
    "Lets put it to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773341f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.grad = 1\n",
    "o._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09144c23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4dbe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1w1x2w2._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36e5f0",
   "metadata": {},
   "source": [
    "So, in order to perform everything automatically, we need to get the list of all nodes, moving from 'o' to the first nodes. \n",
    "\n",
    "We perform this using a topological sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c045be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo = []\n",
    "visited = set()\n",
    "def build_topo(v):\n",
    "    if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "            build_topo(child)\n",
    "        topo.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_topo(o)\n",
    "topo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c115ce6",
   "metadata": {},
   "source": [
    "As you can see, in this order, all the nodes refered by a given node are always traveled **before** the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1\n",
    "build_topo(o)\n",
    "for t in topo[::-1]:\n",
    "    t._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250fa16d",
   "metadata": {},
   "source": [
    "Lets put this inside a method in the Value class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34496bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = 1.0 * out.grad\n",
    "            other.grad = 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * other.data\n",
    "            other.grad = out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8b98c",
   "metadata": {},
   "source": [
    "There is a subtle problem in the code. Lets see in an example. \n",
    "\n",
    "Which is the expected value of the grad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a; b.label = 'b'\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4a907",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "db/da = d(a+a)/da = da/da + da/da = 1 + 1 = 2\n",
    "\n",
    "The problem with our code is that grad values are set, instead of updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68aefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610298e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label='a')\n",
    "b = a + a; b.label = 'b'\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabb81e",
   "metadata": {},
   "source": [
    "Lets see it on a more complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e5be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = Value(-2.0, label='a')\n",
    "b = Value(3.0, label='b')\n",
    "d = a * b; d.label='d'\n",
    "e = a + b; e.label='e'\n",
    "f = d * e; f.label='f'\n",
    "\n",
    "f.backward()\n",
    "draw_dot(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be192117",
   "metadata": {},
   "source": [
    "You can check by hand that everything is working perfectly. Lets add a final manual check!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ce934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    h = 0.001\n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L1 = f.data\n",
    "    \n",
    "    a = Value(-2.0+h, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L2 = f.data \n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a93b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check():\n",
    "    h = 0.001\n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L1 = f.data\n",
    "    \n",
    "    a = Value(-2.0, label='a')\n",
    "    b = Value(3.0+h, label='b')\n",
    "    d = a * b; d.label='d'\n",
    "    e = a + b; e.label='e'\n",
    "    f = d * e; f.label='f'\n",
    "    L2 = f.data \n",
    "    \n",
    "    print((L2 - L1) / h)\n",
    "    \n",
    "check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e31d24",
   "metadata": {},
   "source": [
    "## Decomposing tanh by its components\n",
    "\n",
    "tanh(x) = (e** (2*x)-1) / (e** (2*x)+1)\n",
    "\n",
    "Lets modify Value in order to use use the tanh definition. We introduce some modifications in order to deal with adding a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e8afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        print(self.data, other.data)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3d2760",
   "metadata": {},
   "source": [
    "Lets add now the exponentiation method, with the proper derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373f566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        print(self.data, other.data)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b323bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(2.0)\n",
    "a.exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0488d087",
   "metadata": {},
   "source": [
    "For the division, we will introduce something more general.\n",
    "\n",
    "a / b = a * (1/b) = a * b**-1\n",
    "\n",
    "So, we need the power operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        print(self.data, other.data)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        assert isinstance(exponent, (int, float)), \"Only support int and float for now\"\n",
    "        out = Value(self.data ** exponent, (self, ), f'**{exponent}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * exponent * self.data ** (exponent -1)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other ** -1\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(4.0)\n",
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7127924",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(8.0)\n",
    "b = Value(2.0)\n",
    "a/b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9c5edf",
   "metadata": {},
   "source": [
    "For completness, lets add the substraction and negation operands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827b9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        \n",
    "        self._backward = lambda: None\n",
    "        \n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + -other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self*other\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        assert isinstance(exponent, (int, float)), \"Only support int and float for now\"\n",
    "        out = Value(self.data ** exponent, (self, ), f'**{exponent}')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad = out.grad * exponent * self.data ** (exponent -1)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        return self * other ** -1\n",
    "    \n",
    "    def tanh(self):\n",
    "        out = Value(np.tanh(self.data), (self,), 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - out.data**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), 'exp')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data\n",
    "            \n",
    "        out._backward = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.grad = 1\n",
    "        build_topo(self)\n",
    "        for t in topo[::-1]:\n",
    "            t._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb27988",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(8.0)\n",
    "b = Value(5.0)\n",
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ed599",
   "metadata": {},
   "outputs": [],
   "source": [
    "-a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e39fa",
   "metadata": {},
   "source": [
    "Lets go back to our example, in order to modify the tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b63da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "o = n.tanh(); o.label='o'\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c229c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh(x) = (e** (2x)-1) / (e**(2*x)+1)\n",
    "\n",
    "# inputs x1, x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1, w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias\n",
    "b = Value(6.8812735870195432, label='b')\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label = 'x1w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2w2'\n",
    "\n",
    "x1w1x2w2 = x1w1+x2w2; x1w1x2w2.label = 'x1w1+x2w2'\n",
    "n = x1w1x2w2+b; n.label='n'\n",
    "\n",
    "# o = n.tanh(); o.label='o'\n",
    "ex = (n*2).exp(); ex.label='exp(2**n)'\n",
    "o = (ex -1) / (ex + 1); o.label=o\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3a7ad",
   "metadata": {},
   "source": [
    "This example shows a very important point: **The level of details in the components is up to your needs. You can assemble complex behaviours as units, and you only need to provide the gradient of the complex unit**.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa698c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
